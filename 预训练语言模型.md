# 预训练语言模型

根据语言模型分类：

- **单向**特征表示的**自回归**预训练语言模型，统称为**单向模型**：

- - ELMO/ULMFiT/SiATL/GPT1.0/GPT2.0；

- **双向**特征表示的**自编码**预训练语言模型，统称为**BERT系列模型：**

- - (BERT/MASS/UNILM/ERNIE1.0/ERNIE(THU)/MTDNN/ERNIE2.0/SpanBERT/RoBERTa)

- **双向**特征表示的**自回归**预训练语言模型：

  - **XLNet**；

## 1.不同角度看预训练语言模型

- 不同的特征抽取机制

- - RNNs：ELMO/ULMFiT/SiATL；
  - Transformer：GPT1.0/GPT2.0/BERT系列模型；
  - Transformer-XL：XLNet；

- 不同的预训练语言目标

- - 自编码（AutoEncode）：BERT系列模型；
  - 自回归（AutoRegression）：单向模型（ELMO/ULMFiT/SiATL/GPT1.0/GPT2.0）和XLNet；

- BERT系列模型的改进

- - 引入常识：ERNIE1.0/ERNIE(THU)/ERNIE2.0（简称为“ERNIE系列”）；
  - 引入多任务学习：MTDNN/ERNIE2.0；
  - 基于生成任务的改进：MASS/UNILM；
  - 不同的mask策略：WWM/ERNIE系列/SpanBERT；
  - 精细调参：RoBERTa；

- 特征表示（是否能表示上下文）：

- - 单向特征表示：单向模型（ELMO/ULMFiT/SiATL/GPT1.0/GPT2.0）；
  - 双向特征表示：BERT系列模型+XLNet；



## 2.基于深度学习的NLP特征抽取机制有哪些？各有哪些优缺点？

1）能否处理长距离依赖问题

**长距离依赖建模能力**： Transformer-XL > Transformer > RNNs > CNNs

- - MLP：不考虑序列（位置）信息，不能处理变长序列，如NNLM和word2vec；
  - CNNs：考虑序列（位置）信息，不能处理长距离依赖，聚焦于n-gram提取，pooling操作会导致序列（位置）信息丢失；
  - RNNs：天然适合处理序列（位置）信息，但仍不能处理长距离依赖（由于BPTT导致的梯度消失等问题），故又称之为“较长的短期记忆单元(LSTM)”；
  - Transformer/Transformer-XL：self-attention解决长距离依赖，无位置偏差；

2）前馈/循环网络 or 串行/并行计算

- - MLP/CNNs/Transformer：前馈/并行
  - RNNs/ Transformer-XL：循环/串行：

3)  计算时间复杂度

​		CNN:	O($k\times n\times d^2$)

​		RNN:	O($n\times d^2$)

​		self attention:	O($n^2d$)



## 3.自回归和自编码语言模型各有什么优缺点？

## 4.ELMo模型和优缺点

- 要点：

- - 引入双向语言模型，其实是2个单向语言模型（前向和后向）的集成；
  - 通过保存预训练好的2层biLSTM，通过特征集成或finetune应用于下游任务；

- 缺点：

- - 本质上为自回归语言模型，只能获取单向的特征表示，不能同时获取上下文表示；
  - LSTM不能解决长距离依赖。

- 为什么不能用biLSTM构建双向语言模型？

- - 不能采取2层biLSTM同时进行特征抽取构建双向语言模型，否则会出现**标签泄漏**的问题；因此ELMO前向和后向的LSTM参数独立，共享词向量，独立构建语言模型；

## 5.ULMFit模型和优缺点

- ULMFiT要点：

- - 3阶段：LM预训练+精调特定任务LM+精调特定分类任务；
  - 特征抽取：3层AWD-LSTM；
  - 精调特定分类任务：逐层解冻；

- SiATL要点：

- - 2阶段：LM预训练+特定任务精调分类任务（引入LM作为辅助目标，辅助目标对于小数据有用，与GPT相反）；
  - 特征抽取：LSTM+self-attention；
  - 精调特定分类任务：逐层解冻；

- 都通过一些技巧解决finetune过程中的灾难性遗忘问题：如果预训练用的无监督数据和任务数据所在领域不同，逐层解冻带来的效果更明显；

## 6.GPT1.0和GPT2.0模型和优缺点

- GPT1.0要点：

- - 采用Transformer进行特征抽取，首次将Transformer应用于预训练语言模型；
  - finetune阶段引入语言模型辅助目标（辅助目标对于大数据集有用，小数据反而有所下降，与SiATL相反），解决finetune过程中的灾难性遗忘；
  - 预训练和finetune一致，统一2阶段框架；

- GPT2.0要点：

- - 没有针对特定模型的精调流程：GPT2.0认为预训练中已包含很多特定任务所需的信息。
  - 生成任务取得很好效果，使用覆盖更广、质量更高的数据；

- 缺点：

- - 依然为单向自回归语言模型，无法获取上下文相关的特征表示；

## 7.Bert模型和优缺点

- 优点：能够获取上下文相关的双向特征表示（**BERT的最大亮点**）；

- 缺点：

- - 生成任务表现不佳：预训练过程和生成过程的不一致，导致在生成任务上效果不佳；
  - 采取独立性假设：没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计（不是密度估计）；
  - 输入噪声[MASK]，造成预训练-精调两阶段之间的差异；
  - 无法文档级别的NLP任务，只适合于句子和段落级别的任务；



### 7.1 Bert擅长哪些任务？

1. 适合句子和段落级别的任务，不适用于文档级别的任务（如长文本分类）；
2. 适合处理文本语言本身就能处理好的任务（如QA/机器阅读理解），不依赖于额外特征（如推荐搜索场景）；
3. 适合处理高层语义信息提取的任务，对浅层语义信息提取的任务的提升效果不大（如文本分类/NER，文本分类关注于“关键词”这种浅层语义的提取）；
4. 适合处理句子/段落的匹配任务，因为BERT在预训练任务中引入NSP；因此，在一些任务中可以构造辅助句（类似匹配任务）实现效果提升（如关系抽象/情感挖掘等任务）；
5. 不适合处理NLG任务，因为BERT在生成任务上效果不佳；



### 7.2 Bert用在中文上时，是基于字好还是基于词好？

1. 如果基于“词输入”，会出现OOV问题，会增大标签空间，需要利用更多语料去学习标签分布来拟合模型。
2. 随着Transfomer特征抽取能力，分词不再成为必要，词级别的特征学习可以纳入为内部特征进行表示学习。



### 7.3 Bert为什么不适用于自然语言生成任务？

1. 由于BERT本身在预训练过程和生成过程的不一致，并没有做生成任务的相应机制，导致在生成任务上效果不佳，不能直接应用于生成任务。
2. 如果将BERT或者GPT用于Seq2Seq的自然语言生成任务，可以分别进行预训练编码器和解码器，但是编码器-注意力-解码器结构没有被联合训练，BERT和GPT在条件生成任务中只是次优效果。



### 7.4 针对BERT原生模型，后续的BERT系列模型是如何改进【mask策略】的？

原生BERT模型：按照subword维度进行mask，然后进行预测；局部的语言信号，缺乏全局建模的能力。

- BERT WWM(Google)：按照whole word维度进行mask，然后进行预测；

- ERNIE等系列：建模词、短语、实体的完整语义：通过先验知识将知识（短语、词、实体）进行整体mask；引入外部知识，按照entity维度进行mask，然后进行预测；

- SpanBert：不需要按照先验的词/实体/短语等边界信息进行mask，而是采取随机mask：

- - **采用Span Masking**：根据几何分布，随机选择一段空间长度，之后再根据均匀分布随机选择起始位置，最后按照长度mask；通过采样，平均被遮盖长度是3.8 个词的长度；
  - **引入Span Boundary Objective**：新的预训练目标旨在使被mask的Span 边界的词向量能学习到 Span中被mask的部分；新的预训练目标和MLM一起使用；

- 注意：BERT WWM、ERNIE等系列、SpanBERT旨在**隐式地学习**预测词（mask部分本身的强相关性）之间的关系[[23\]](https://zhuanlan.zhihu.com/p/76912493#ref_23)，而在 XLNet 中，是通过 PLM 加上自回归方式来**显式地学习**预测词之间关系；

## 8.Roberta模型和优缺点

- 修改了超参数：将adam的 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta_2) 参数从0.999改为0.98
- 加入了混合精度
- 加大batch size：**从BERT的256改为2K甚至8K，训练步数从1M降到500K**
- 在更长的序列上训练，修改输入格式：FULL-SENTENCES+移除NSP任务
- 将BERT静态遮掩改为动态遮掩
- 增加新的预训练数据集CC-NEWS，语料从16G文本到160G文本
- Text Encoding：采用更大的byte-level的BPE词典

## 9.Ernie模型和优缺点

- 在预训练阶段引入知识（实际是预先识别出的实体），引入3种[MASK]策略预测：

- - Basic-Level Masking： 跟BERT一样，对subword进行mask，无法获取高层次语义；
  - Phrase-Level Masking： mask连续短语；
  - Entity-Level Masking： mask实体；

## 10.XLNet模型和优缺点





























