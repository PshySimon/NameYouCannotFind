# 语言模型

## 1.传统语言模型

+ 前向语言模型

## 2.NNLM

+ 神经网络语言模型

## 3.Word2Vec

+ 神经网络语言模型

## 4.FastText

+ 神经网络语言模型

## 5.ULMFit



## 6.ELMo

+ 结构：一层Embedding+两层单向LSTM

+ 双向语言模型(自回归语言模型)

+ 损失函数
  $$
  loss=\sum_{k=1}^N(\log{p(t_k|t1, ..., t_{k-1};\Theta_x,\overrightarrow{\Theta}_{LSTM},\Theta_s)}+\log{p(t_k|t_{k+1},...,t_N);\Theta_x,\overleftarrow{\Theta}}_{LSTM},\Theta_s)
  $$
  分别用两个独立的单向LSTM对句子两个方向去建模，其中$\Theta_x，\Theta_s$是共享的参数，$\overleftarrow{\Theta}_{LSTM},\overrightarrow{\Theta}_{LSTM}$分别是LSTM正向和反向的参数。损失函数是分类损失函数，用于预测上一个或者下一个单词。

+ 训练好后如何使用？

  ELMo有三层结构，每一层都有输出，将第一层的原始词向量$X$与两层LSTM的隐藏层输出$H$，然后这三层都有各自的权重，权重是学习而来的，根据权重做加权和得到词向量输入到下接任务中。

+ 最大贡献：解决了一词多义问题

  对于前面的语言模型，都是一个单词对应一个向量，但是语言中的单词都是一个单词对应多重含义，在不同的上下文的情况下有着不同的含义，所以词向量的表示还依赖于上下文。ELMo模型生成的词向量不光带有原始的词向量，还经过特定上下文的计算得到了上下文相关的词向量的表示，对于一词多义有了较好的解决方案

+ 为什么ELMo能够区分一词多义？

  经过LSTM抽取特征之后，得到的单词会受到上下文的影响，不同的上下文会给使单词强化某种语义，弱化其他语义，这样就能达到区分多义词的效果了。

+ 为什么要使用两个单向LSTM而不使用BiLSTM？

  用双向语言模型会看到自己或者看到答案

## 7.GPT1.0

## 8.Bert

+ 结构：三层Embedding+N层Transformer的Encoder结构
+ 自编码语言模型
+ 损失函数：

## 9.GPT2.0

