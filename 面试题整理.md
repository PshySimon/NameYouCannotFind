# 面试题整理

**通用基础**

+ 一般会对数据进行哪些处理和分析？
+ 了解特征工程吗？使用过/知道哪些方法？
+ 训练集/测试集划分，交叉验证的原理？
+ 介绍训练集、验证集、测试集的使用情况，以及实践中遇到的一些问题和技巧？
+ 是否遇到过样本不均衡问题，是如何解决的？
+ 知道哪些样本不均衡的方法，各自的优缺点，自己实现了哪些，效果如何？
+ 当训练数据与真实数据分布不一致时，线上预测效果是否会出现偏差？
+ 多分类任务中遇到某几个类区分不开的情况是如何处理的？
+ 如何根据训练结果判断什么时候过拟合什么时候欠拟合？
+ 过拟合的解决方法，优缺点，自己实现了哪些，效果如何？
+ 知道哪些损失函数，优缺点及适用问题？
+ 写一个CrossEntropy的公式，并说明它的物理意义？
+ f1值的计算公式？
+ 介绍一下precision和recall？
+ auc原理，为什么更适用于排序问题？
+ 有哪些正则化的方法？
+ 常见的激活函数及他们的特点
+ 说说知道的优化器
+ sgd和Adam的公式
+ Dropout原理和代码
+ BatchNorm
+ 什么是高方差、高偏差吗？遇到时应如何解决？
+ 讲一下cnn中的padding，有什么用？池化有什么用呢？
+ 神经网络初始化的方式
+ 神经网络梯度爆炸/消失的原因
+ beam search
+ 一阶优化器、二阶优化器
+ 过拟合怎么解决
+ 标签平滑
+ 交叉熵、相对熵
+ 维特比算法
+ 



**自然语言处理**

+ 根据自己的理解介绍一下NLP的发展史？
+ 对统计学方法（非深度学习）在NLP方面的应用了解多少？
+ 对文本分类的最新方法了解多少？
+ 是否了解label embedding？
+ TF-IDF的原理和公式？
+ TF-IDF的弊端是什么？
+ Word2Vec原理，词向量是如何训练出来的？
+ Word2Vec参数量计算？
+ CBOW与Skip-gram的训练过程，以及使用的加速技巧？
+ 从原理的角度解释一下为什么Skip-gram效果一般比CBOW更好？
+ Word2Vec有什么缺点？
+ 画一下GRU的单元结构？
+ 介绍一下RNN、LSTM、Transformer各自的优缺点？
+ 介绍一下梯度爆炸/梯度消失的解决办法？
+ RNN为什么容易出现梯度爆炸/梯度消失的问题，能否使用一些技巧缓解？
+ LSTM如何解决梯度消失/爆炸？
+ 实践中如何解决梯度爆炸问题，超参数如何设置的？
+ 设置梯度裁剪阈值时，有没有尝试通过参数分布来计算，而不是直接使用超参数？
+ LSTM的图结构以及公式，以及LSTM的流程，与GRU的区别。面对长文本有什么解决办法？
+ 介绍一下LSTM的原理，hidden_state和outputs的关系？
+ LSTM的激活函数是什么？能否使用ReLU？
+ LSTM的参数量以及初始化方式？
+ LSTM里面有哪些门，为什么用这些门？
+ RNN输入长度不一致如何处理？
+ LSTM解决了RNN的什么问题？为什么？
+ 有没有见过其他类似LSTM和GRU这种门控机制的网络架构？
+ TextCNN、TextRNN的原理和细节？
+ TextCNN的卷积核大小，为什么要对文本进行卷积，卷积核大小选取的标准？
+ TextCNN中卷积核的物理意义是什么，提取全局特征还是局部特征？
+ TextCNN和图像中的CNN最大的区别是什么？
+ Transformer原理以及自己的理解（相比于之前的方法有哪些优势）？
+ Transformer激活函数的位置？
+ Transformer的时间复杂度和空间复杂度？
+ Transformer的Decode阶段都哪些地方使用了PAD？
+ Transformer中Attention的Softmax中根号d代表什么，作用是什么，为什么？
+ 讲一讲BERT的结构和原理？
+ BERT为什么要用CLS和MLM两种训练方式？
+ BERT是如何利用位置信息的（如何训练位置向量），不同方式之间有什么区别？
+ BERT相比于Word2Vec有哪些优势？
+ BERT为什么可以解决一词多义问题？
+ 相比于BERT得到的各个词向量，直接使用得到的句向量有什么优点？
+ 使用BERT词向量时，是将文本输入得到output还是直接使用embedding向量，为什么？
+ 介绍一下ELMo、BERT和GPT之间的区别？
+ Self-Attention/Attention/Multi head-Attention/Mask-Attention原理、实现细节？
+ 自己对Attention的理解（带来了哪些提升和优势）？
+ Attention如何参考词的位置信息？
+ 为什么要使用Multi head-Attention？
+ Seq2Seq结构原理及实现细节？
+ LSTM使用的优化器是什么？
+ 介绍一下Adam优化器？
+ Adam和SGD的区别是什么？
+ 文本截断补齐的实现细节（Mask使用细节）？
+ 了解XLnet吗？为什么XLnet效果比bert效果好？
+ BERT的mask 机制，有什么缺点，针对这个缺点有提出XLNet
+ seq2seq有使用过吗？讲一讲？



+ 文本相似度有哪些度量的方法？
+ HMM和CRF有哪些区别？





+ BIO机制原理及实现细节？
+ 问答场景中如何使用向量cos相似度构建位置信息向量？
+ 将不同模型的Embedding直接拼接会带来什么问题，为什么？
+ 是否了解其他提取位置信息的方法，介绍一下？
+ Softmax原理及实现细节？
+ Softmax词典输出维度太大如何解决？
+ Copy mechanism、Pointer Network原理及实现细节？
+ 为什么要使用Copy操作，能带来什么好处？
+ 为什么使用Copy的分布相加而不是直接使用Pointer Network取最大作为输出？
+ Teacher Forcing原理及实现细节？
+ Teacher Forcing的优缺点，什么场景下适合使用？
+ 为什么使用Attention、Copy和Teacher Forcing这些技巧，自己的想法还是借鉴别人的？
+ Beam搜索时，是topk一个一个输入还是一起输入到下一个单元？
+ 词向量预训练细节（梯度打开时机）？
+ 是否了解其他预训练模型，介绍一下？
+ Fine-Tuning原理及实现细节？
+ ROUGE-L评价指标的原理？
+ ROUGE-2评价指标的原理？
+ BLEU和ROUGE评价指标的区别，如何根据具体任务进行选择？
+ 如何在给定一部分问答对数据的情况下，进行文本数据扩充？
+ 知道哪些激活函数，都有什么优缺点？
+ 计算两个两段文本的相似度该如何设计模型，如何部署，如何设置loss进行梯度回传？
+ 如何从文本的角度计算Python中两个def函数间的相似度？
+ 机器翻译算不算文本生成任务，为什么？
+ 模型融合使用了哪些模型，如何实现的？
+ 平时有没有关注自然语言处理领域的最新热点，并介绍一下目前最新的热点？
+ 谈谈对预训练模型和自监督学习的理解？
+ 是否了解知识图谱，介绍一下？
+ 





**机器学习**

+ 介绍一下信息熵、信息增益与信息增益比？
+ 介绍一下GDBT原理
+ XGBoost和Random Forest的区别？
+ Random Forest随机提现在哪方面？
+ 说说XGBoost和GDBT的区别？
+ 介绍一下XGBoost的原理，有哪些优点？
+ XGBoost每个节点是如何分裂的？
+ XGBoost特征选择的方式？
+ XGBoost为什么用CART树？
+ XGBoost使用细节（特征维度太高的时候，为什么面临输入问题，如何解决）？
+ 调用XGBoost使用的哪个库？
+ 介绍一下LightGBM？
+ LightGBM和XGB有什么差异，带来了哪些改进，如何做到的？
+ 是否了解其他的集成模型，介绍一下？
+ 相比于直接使用传统分类器，集成学习的方法有哪些优点？
+ 树模型和其他预测模型之间最大的区别是什么？
+ 为什么树模型不需要对特征进行标准化处理？
+ 为什么一般预测模型要对特征进行标准化处理？
+ 不进行标准化处理会带来哪些问题，为什么？
+ 说说Logistic Regression和SVM的区别？
+ 介绍一下SVM的原理？
+ SVM的损失函数是什么，编程实现一下？
+ TSVM半监督算法原理及实现细节？
+ 为什么SVM要求解它的对偶问题？
  + 因为对偶问题能降低求解的时间复杂度。
+ 聚类的算法有哪些？评价方法？优化算法？
+ Kmeans与kNN什么区别？
+ Kmeans的缺点？如何改善？





**算法题**

+ 编辑距离
+ 二叉树层次遍历
+ 给定数组按照给定规则排序
+ 翻转字符串，思考边界问题

```python
def reverseString(s):
    """
    使用双指针，头尾指针元素相互交换
    :param s:
    :return:
    """
    i,j = 0, len(s)-1
    while i<=j:
        # temp =s[i]
        # s[i] = s[j]
        # s[j] = temp
        s[i],s[j] = s[j],s[i] #交换值
        i += 1
        j -= 1
    return s
```

+ 序列化二叉树（剑指Offer）
+ 翻转m到n之间的链表
+ 一个链表，长度未知（设为n），只能遍历一次，取出k个元素，使得每一个元素取到的可能性一样？（蓄水池抽样）
+ 从一个数据流取出n个数据，保证每个数据被抽到的概率相同，问抽取策略？（蓄水池抽样）
+ [41. 缺失的第一个正数](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/first-missing-positive/)
+ [347. 前 K 个高频元素](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/top-k-frequent-elements/)
+ [334. 递增的三元子序列](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/increasing-triplet-subsequence/)
+ [468. 验证IP地址](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/validate-ip-address/)
+ 基于辞典的正向最长匹配分词
+ [69. x 的平方根](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/sqrtx/)（迭代和递归两种方法实现）**
+ [34. 在排序数组中查找元素的第一个和最后一个位置](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/)
+ [剑指 Offer 09. 用两个栈实现队列](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/yong-liang-ge-zhan-shi-xian-dui-lie-lcof/)
+ [Leetcode 503. 下一个更大元素 II](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/next-greater-element-ii/)
+ [142. 环形链表 II](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/linked-list-cycle-ii/)
+ [138. 复制带随机指针的链表](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/copy-list-with-random-pointer/)
+ [25. K 个一组翻转链表](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/reverse-nodes-in-k-group/) **
+ [199. 二叉树的右视图](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/binary-tree-right-side-view/)
+  [剑指 Offer 28. 对称的二叉树](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/dui-cheng-de-er-cha-shu-lcof/)
+ [951. 翻转等价二叉树](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/flip-equivalent-binary-trees/)
+ [415. 字符串相加](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/add-strings/)包含小数
+ 将嵌套列表变为嵌套元组
+ [300. 最长上升子序列](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/longest-increasing-subsequence/) 
+ [1143. 最长公共子序列](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/longest-common-subsequence/)输出最长公共序列
+ [5. 最长回文子串](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/longest-palindromic-substring/)
+  [887. 鸡蛋掉落](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/super-egg-drop/)
+  [200. 岛屿数量](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/number-of-islands/)
+ 查找同义词集
+ 拓扑排序
+ 输出子图的拓扑排序
+ 给定一组等式与不等式字符串，判断是否合法，如"a=b","b=c","a!=c"是非法的。
+ [25. K 个一组翻转链表](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/reverse-nodes-in-k-group/)
+ [5. 最长回文子串](https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/longest-palindromic-substring/)
+ 324.涓流数组
+ 求数组中乘积最大的两数
+ 大量数据，亿为单位，找出与给定数据最相似的一个？（什么鬼东西...）
+ 给出前序遍历和中序遍历，重建二叉树
+ 旋转链表
+  N\*3的木板，用1\*3的砖块铺满，有多少种方案？
+ 旋转数组找最大值
+ （字节祖传题）三数之和
+ 实现kmeans
+ 887.super egg drop
+ 



**编程基础**

+ dict和list的区别，遍历复杂度
+ 二叉树、二叉搜索树时间复杂度
+ 面向对象编程的关键特性？
+ Pandas，输出年龄大于10的数据？
+ 浅拷贝与深拷贝？
+ 垃圾回收机制？
+ Python的三元运算符
+ java接口与抽象类的区别
+ 





**概率论**

+ x, y是独立的随机变量，方差期望已知，那么如何求 xy 的方差
+ 熵的定义，并复述公式
+ PCA的原理及公式
+ 





数据结构

+ 完全二叉树
+  一个具有N节点的完全二叉树深度是多少 [log2n]+1
+ 链表与线性表的区别
+ 



场景题

+ 实际场景下做softmax容易出现一些问题，怎么解决（面试的时候没明白什么意思，面试结束后询问，他是说实际场景做softmax很容易出现下溢问题，这个可以用每个维度减去一个固定值就可以了）
+ 



























