# 优化算法

## 1.SGD



+ 一维梯度下降算法（Gradient Descent）

  一阶泰勒公式：

  $$
  f(x + \epsilon) \approx f(x)+\epsilon f'(x)
  $$
  寻找一个常数$\eta > 0$，使得$|\eta f'(x)|$足够小，来替换上式的$\epsilon$
  得到：
  $$
  f(x -\eta f'(x)) \approx f(x)-\eta f'(x)^2
  $$
  
  
  用$-\eta f'(x)$来替换$\epsilon$主要是为了做梯度下降，求极小值
  如果导数$f'(x)$不为0，那么$\eta f'(x)^2$一定大于0
  故：
  $$
  f(x - \eta f'(x)) \leq f(x)
  $$
  也就是说通过梯度下降法迭代，每次目标函数值都在减小
  
  + GD算法有一个超参数，即迭代步长--学习率：
  
    + 小学习率情况下：
  
    ![image-20201013184423169](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013184423169.png)
  
    + 大学习率情况下：
  
    ![image-20201013184439235](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013184439235.png)





+ 多维梯度下降：

考虑更广义的情况：目标函数的输入为向量，输出为向量。输入为一个d维的向量，$x\in R^d$，$x=[x_1,x_2,...,x_d]^T$，目标函数关于x的梯度是一个由d个偏导组成的向量：
$$
\nabla_xf(x)=[\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2}...\frac{\partial f(x)}{\partial x_d}]^T
$$
定义方向导数：
$$
D_uf(x) = \lim_{h\rightarrow0}\frac{f(x+hu)-f(x)}{h}
$$
根据方向导数性质：
$$
D_uf(x)=\nabla f(x)\cdot u
$$
方向导数$D_uf(x)$给出了f在x上所有可能方向的变化率。为了最小化f，我们需要找到f能被降低的最快方向。
$$
D_uf(x)=||\nabla f(x)||\cdot||u||\cdot cos\theta=||\nabla f(x)||\cdot cos\theta
$$
当$\theta$取值为$\pi$时，$cos\theta$取得最小值-1.故：
$$
x\leftarrow x-\eta \nabla f(x)
$$




+ 随机梯度下降：

在深度学习里，目标函数通常是有关各个样本的损失函数的平均。设$f_i(x)$是有关索引为i的训练数据的损失函数​，n是训练样本数，x是模型的参数向量，目标函数就定义为：
$$
f(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)
$$
目标函数在x处的梯度为：
$$
\nabla f(x)=\frac{1}{n}\sum_{i=1}^n\nabla f_i(x)
$$
如果使用梯度下降，每次迭代计算的开销为O(n)，它随着n线性增长。因此当训练样本数很大时，梯度下降每次迭代的计算开销很高。值的强调的是，随机梯度$\nabla f_i(x)$是对梯度$\nabla f(x)$的无偏估计。
$$
E_i\nabla f_i(x)=\frac{1}{n}\sum_{i=1}^nf_i(x)=\nabla f(x)
$$

**无偏估计**：统计学参数估计的一种方法，利用样本来计算出的统计量来估计总体的参数。估计量的数学期望等于被估计参数的真实值，则称该估计量为被估计参数的无偏估计，即无偏性。**意义**：在多次重复下，其平均数接近所估计的参数真值。



+ 小批量随机梯度下降

一次迭代采样一个小批量的样本做梯度下降。
$$
g_t\leftarrow \nabla f_{\Beta_t}(x_{t-1})-\eta_tg_t
$$
基于随机采样得到的梯度的方差在迭代过程中无法减小，因此在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减。

+ 梯度下降和随机梯度下降的比较：

  + 梯度下降的损失值迭代情况

  ![image-20201013195923665](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013195923665.png)

  + 随机梯度下降的损失值迭代情况

  ![image-20201013200044707](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013200044707.png)

可以看出梯度下降收敛比随机梯度下降慢。

在小批量梯度下降法中，批量大小（Batch Size）对网络优化的影响也非常大．一般而言，批量大小不影响随机梯度的期望，但是会影响随机梯度的方差．批量大小越大，随机梯度的方差越小，引入的噪声也越小，训练也越稳定，因此可以设置较大的学习率．而批量大小较小时，需要设置较小的学习率，否则模型会不收敛．学习率通常要随着批量大小的增大而相应地增大.

实现方法：

```python
def sgd(params, lr=1e-3):
    for param in params:
        param.data -= para.grad.data * lr
```



## 2.动量法

+ 梯度下降的问题：

考虑一个输入和输出分别是二维向量$x=[x_1,x_2]^T$和标量的目标函数$f(x)=0.1x_1^2+2x_2^2$，绘制其轨迹为：

```python
import matplotlib.pyplot as plt
import numpy as np

def f(x1, x2):
    return 0.1*x1**2+2*x2**2

x1, x2 = np.meshgrid(np.arange(-5.5, 1.0, 0.1), np.arange(-3.0, 1.0, 0.1))

plt.contour(x1,x2,f(x1,x2))
plt.show()
```

![image-20201013202456115](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013202456115.png)

可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。

尝试使用大一点的学习率时，最优解不断被越过最终发散。

![image-20201013202825208](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013202825208.png)

使用动量法之后，轨迹变得更平滑了。

![image-20201013203023501](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013203023501.png)

+ 指数加权平均

给定超参数$0\leq \gamma < 1$，当前变量$y_t$是上一时间步变量$y_{t-1}$和另一变量$x_t$的线性组合：
$$
y_t=\gamma y_{t-1}+(1-\gamma)x_t
$$
对$y_t$展开：
$$
\begin{aligned}
y_t&=(1-\gamma)x_t+\gamma y_{t-1}\\
&=(1-\gamma)x_t+(1-\gamma)\cdot \gamma x_{t-1}+\gamma^2y_{t-2}\\
&=(1-\gamma)x_t+(1-\gamma)\cdot \gamma x_{t-1}+(1-\gamma)\cdot \gamma^2x_{t-2}+\gamma^3y_{t-3}\\
&...
\end{aligned}
$$
令$n=\frac{1}{1-\gamma}$，那么$(1-\frac{1}{n})^n=\gamma^{\frac{1}{1-\gamma}}$，又因为：
$$
\lim_{n\rightarrow \infty}(1-\frac{1}{n})^n=\frac{1}{e}\approx 0.3679
$$
而$0.95^{20}\approx \frac{1}{e}$，将0.3679当做一个比较小的数，可以近似得到（忽略$\gamma^{\frac{1}{1-\gamma}}$及其更高阶的无穷小）：
$$
y_t\approx0.05\sum_{i=0}^{19}0.95^ix_{t-i}
$$
因此在实际中我们常常将$y_t$看做是对最近$\frac{1}{1-\gamma}$个时间步的$x_t$值的加权平均。如：当$\gamma=0.95$时，$y_t$可以被看做对最近20个时间步的$x_t$值的加权平均；而且，离当前时间步t跃进的$x_t$获取的权重越大。



+ 动量法

$$
v_t\leftarrow \gamma v_{t-1}+\eta_tg_t\\
x_t\leftarrow x_{t-1}-v_t
$$

对其做变形：
$$
v_t\leftarrow \gamma v_{t-1}+(1-\gamma)(\frac{\eta_t}{1-\gamma}g_t)
$$
即给定超参数$\gamma$之后，动量实际上是对梯度做了指数加权移动平均。在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。

```python
def sgd_momentum(params, states, momentum, lr):
    for p, v in zip(params, states):
        v.data = momentum * v.data + lr * p.grad.data
        p.data -= v.data
        
        
# pytorch的SGD自带了momentum
optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9)
```



## 3.AdaGrad

根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。首先累加梯度的平方：
$$
s_t\leftarrow s_{t-1}+g_t\bigodot g_t
$$
然后调整各个方向的学习率：
$$
x_t\leftarrow x_{t-1}-\frac{\eta}{\sqrt{s_t+\epsilon}}\bigodot g_t
$$
如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。**

```python
optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)
```

SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到）。**对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。**

这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为$s_t$ 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。



## 4.RMSProp

AdaGrad太莽了，做法过于激进，因此考虑到momentum的指数加权平均，不适用AdaGrad用的平方累加，而是：
$$
s_t\leftarrow \gamma s_{t-1} + (1-\gamma)g_t\bigodot g_t
$$
更新方法还是那样：
$$
x_t \leftarrow x_{t-1}-\frac{n}{s_t+\epsilon}\bigodot g_t
$$
此时梯度下降轨迹就平滑多了：

![image-20201013213627936](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201013213627936.png)

```python
optimizer = torch.optim.RMSprop(net.parameters(), lr = 0.01, alpha = 0.9)
```

这里的alpha是超参数$\gamma$的值



## 5.AdaDelta

AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$g_t$按元素平方的指数加权移动平均变量$s_t$*。在时间步0，它的所有元素被初始化为0。给定超参数$0 \leq \rho <1$（对应RMSProp算法中的*$\gamma$），在时间步t>0，同RMSProp算法一样计算
$$
s_t \leftarrow \rho s_{t-1}+(1-\rho)g_t\bigodot g_t
$$
与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\Delta x_t$，其元素同样在时间步0时被初始化为0。我们使用来$\Delta x_{t-1}$计算自变量的变化量：
$$
g'_t\leftarrow\sqrt{\frac{\Delta x_{t-1}+\epsilon}{s_t+\epsilon}}\bigodot g_t
$$
其中ϵ是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：
$$
x_t\leftarrow x_{t-1}-g'_t
$$
最后，我们使用$\Delta x_t00$来记录自变量变化量$g'_t$按元素平方的指数加权移动平均：
$$
\Delta x_t\leftarrow \rho \Delta x_{t-1}+(1-\rho)g'_t\bigodot g'_t
$$
可以看到，如不考虑ϵ的影响，AdaDelta算法跟RMSProp算法的不同之处在于使用$\sqrt{\Delta x_{t-1}}$来替代学习率η。

```python
# lr是在delta被应用到参数更新之前对它缩放的系数
torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06)
```

优点：避免在训练后期，学习率过小；初期和中期，加速效果不错，训练速度快

缺点：还是需要自己手动指定初始学习率，初始梯度很大的话，会导致整个训练过程的学习率一直很小，在模型训练的后期，模型会反复地在局部最小值附近抖动，从而导致学习时间变长



## 6.Adam

首先定义动量$v_t$和指数加权平均变量$s_t$，在时间步0时初始化为0.给定超参数$0 \leq \beta_1 < 1$，算法作者建议为0.9：
$$
v_t \leftarrow \beta_1v_{t-1}+(1-\beta_1)g_t
$$
给定超参数$0\leq\beta_2<1$，算法作者建议设为0.999：
$$
s_t \leftarrow \beta_2s_{t-1}+(1-\beta_2)g_t \bigodot g_t
$$
由于我们将$v_0$和$s_0$中的元素都初始化为0， 在时间步t*我们得到$v_t=(1-\beta_1)\sum_{i=1}^t\beta_1^{t-i}g_i$。将过去各时间步小批量随机梯度的权值相加，得到$(1-\beta_1)\sum_{i=1}^t\beta_1^{t-i}=1-\beta_1^t$。需要注意的是，当t较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\beta_1=0.9$时，$v_1=0.1g_1$。为了消除这样的影响，对于任意时间步t，我们可以将$v_t$*再除以$1-\beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$v_t$和$s_t$均作偏差修正：

**上面用到的公式有：**
$$
\begin{aligned}
(1-\beta)(\beta^{t-1}+\beta^{t-2}+...+\beta^0)\\
&=\beta^{t-1}+\beta^{t-2}+\beta^{t-3}+...+\beta^0-\beta^t-\beta^{t-1}-\beta^{t-2}-...-\beta^1\\
&=\beta^0-\beta^t\\
&=1-\beta^t
\end{aligned}
$$
偏差修正：
$$
\hat{v_t} \leftarrow \frac{v_t}{1-\beta_1^t},\\
\hat{s_t}\leftarrow \frac{s_t}{1-\beta_2^t}
$$
然后重新调整：
$$
g'_t\leftarrow \frac{\eta\hat{v_t}}{\sqrt{\hat{s_t}+\epsilon}}
$$
最后迭代得到：
$$
x_t \leftarrow x_{t-1}-g'_t
$$

```python
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
```





### 6.1 AdamW

AdamW在Adam的基础上增加了weight decay，weight decay是在梯度更新时增加了一项：
$$
x_{t+1}=x_t-\alpha\nabla f_t(x_t) - wx_t
$$
对比L2正则化，它是直接在损失函数上增加了L2范数项：
$$
loss(x_t) = f_t(x) + \frac{1}{2}w||x||_2^2
$$
对于标准SGD来说，两者等价，最终结果一致，但是对于Adam来说两者不等价。

使用Adam优化带L2正则的损失并不有效。如果引入L2正则项，在计算梯度的时候会加上对正则项求梯度的结果。那么如果本身比较大的一些权重对应的梯度也会比较大，由于Adam计算步骤中减去项会有除以梯度平方的累积，使得减去项偏小。按常理说，越大的权重应该惩罚越大，但是在Adam并不是这样。而权重衰减对所有的权重都是采用相同的系数进行更新，越大的权重显然惩罚越大。在常见的深度学习库中只提供了L2正则，并没有提供权重衰减的实现。这可能就是导致Adam跑出来的很多效果相对SGD with Momentum偏差的一个原因。

```python
torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)
```















