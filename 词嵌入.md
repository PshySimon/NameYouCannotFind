# 词嵌入

## 1.One-Hot

**缺点：**

+ 数据稀疏和维度灾难
+ 没有考虑句子中字的顺序
+ 没有考虑句子中字的重要程度



## 2.Bag-of-Words

**符号及其含义：**

​	D表示文档的集合，d表示一篇文档，文档是字符的集合，所有文档字符的集合为C，文档字符集合中文字的数量为n，文档向量表示成n维向量，每个位置表示的是字出现的频率

**缺点：**

+ 数据稀疏和维度灾难
+ 没有考虑字词的顺序



**改进：**

加入n-gram

​	Bag-of-Words+n-gram



## 3.TFIDF

**思想**：

​	词袋模型中统计的是词出现的频率，一般词出现频率越高表示词这文档中越重要，但是有挺多词如：“的”，“地”等一些词没有意义，词频也很高，这种词一般在很多文档中都很常见，所以将词频除以逆文档频率即可。

**符号及其含义：**

​	文档数为d，词为w，N表示语料中文档的数量，N(w)表示词w出现在多少个文档中

​	TF(Term Frequency)=$\frac{count(w)}{d}$

​	IDF(Inverse Document Frequency)=$log\frac{N+1}{N(w)+1}+1$

**计算公式：**
$$
TFIDF = TF*IDF
$$
**缺点：**

+ 没把词与词之间的关系表达出来
+ 词还是无序的



**改进**：

加入n-gram



## 4.离散表示存在的问题

+ 无法衡量词之间的关系
+ 维度灾难和数据稀疏
+ 加入n-gram之后呈指数增长
+ 没有词序的关系



## 5.分布式表示

用一个词附近的词表示这个词的意思。

## 5.1共现矩阵

采取滑窗机制，矩阵中各个位置表示词表的索引，值代表两个词在在滑窗中同时出现的次数

词共现矩阵用于主题模型(LSA)。

## 5.2NNLM

+ 任务：输入文本为n-1个词，输出最后一个字/词（在词表中的）出现的概率
+ 输入：输入层是一个one-hot向量，形状为[V, 1]，V是词汇表大小，只有表示这个词的位置上为1，其余为0
+ 投影层：投影层C是一个[D, V]的矩阵，D是分布式表示的维度，用C与输入相乘即可得到[D,1]的向量
  + 将得到的向量按行拼接，得到[n*D, 1]的向量
+ 隐藏层：利用全连接层将得到的向量从[n\*D, 1]投射成[H, 1]，即隐藏层权重形状为[H, n\*D]，激活函数为tanh
+ 输出层：利用Softmax将输出向量[H, 1]归一化，选取概率最大的值作为预测词语的概率

通过训练，最后只取隐藏层，NNLM的训练目标是语言模型，但是能得到词向量，词向量只是副产物。

损失函数如下（交叉熵损失函数）：
$$
L = \frac{1}{n}P(w_5|w_1,w_2,w_3,w_4)
$$

## 5.3Word2Vec

Word2Vec有两种结构：CBOW和Skip-gram。其中CBOW是已知上下文来推测某个词出现的概率；而Skip-gram是已知某个词来推测上下文。

两种模型网络架构都包含：输入层、投影层和输出层，那么两种架构优化的损失函数分别是：
$$
L=\sum_{w\in C}log(p(w|Context(w)))\\
L=\sum_{w\in C}log(p(Context(w)|w))
$$

+ CBOW模型

  这里假设所要预测的单词w的上下文为前后共C个单词，输入的样本为(Context(w), w)，然后的处理过程为：

  + 输入层：将C个上下文的单词映射成C个one-hot向量

  $$
  v(Context(w)_1), v(Context(w)_2), ..., v(Context(w)_2c)\in R^m\\
  m为词向量维度emb\_dim
  $$

  + 投影层：与NNLM不同的是，CBOW将这C个向量没有做拼接，而是累加

  $$
  X_w=\frac{1}{C}W\sum_{i=1}^{C}v(Context(w)_i)\in R^m
  $$

  + 输出层：正常的思路应该是乘上一个矩阵，将向量$X_w$映射成vocab_size维的向量，然后通过softmax得出概率最大的词；将得到的向量乘上隐藏层的权重W并经过激活函数得到U，设权重的第j列为$v_j^T$，与词向量相乘后得到：$u_j=v_j^T\cdot X_w$，最后输出为y，其中第j个值为$y_j$

  $$
  y_j = \frac{exp(u_j^*)}{\sum_{i=1}^{V}exp(u'_i)}
  $$

  + 损失函数：（当单词索引为正确预测的词时为j*，否则为j）

  $$
  \begin{aligned}
  Loss&=-log(p(w|Context(w)))\\
  &=-(u_j^*-log\sum_{i=1}^{V}exp(u'_j))\\
  &=log\sum_{i=1}^{V}exp(u'_j))-u_j^*
  \end{aligned}
  $$

  + 梯度传播：

  $$
  \begin{aligned}
  \frac{\partial Loss}{\partial u_j}&=\frac{exp(u_j)}{\sum_{i=1}^{V}exp(u'_j))}-t_j(若j=j*，则t_j=1；否则为0)\\
  &=y_j-t_j\\
  &=e_j
  \end{aligned}
  $$

  ​	接下来对隐藏层求导：
  $$
  \begin{aligned}
  \frac{\partial Loss}{\partial w_{i,j}}&=\frac{\partial Loss}{\partial u_j}\cdot \frac{\partial u_j}{\partial w_{i,j}}\\
  &=e_j\cdot v_i
  \end{aligned}
  $$
  ​	梯度更新：
  $$
  w_{i,j}'=w_{i,j} - \eta\cdot e_j\cdot h_i(for j=1, 2, ... , V)
  $$
  

















































