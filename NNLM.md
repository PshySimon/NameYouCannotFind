# NNLM

## 1.概念

![image-20210218185642264](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210218185642264.png)

语言模型，单词$w_1,w_2,...,w_n$共同出现构成句子的概率为：
$$
p(S)=p(w_1,w_2,...w_n)=p(w_1)p(w_2|w_1)p(w_3|w_2,w_1)...p(w_n|w_1,w_2,...,w_{n-1})
$$
但是句子越长像$p(w_n|w_1,w_2,...,w_{n-1})$的参数空间很大，难以计算，很多词汇概率的组合为0，数据非常稀疏，整个句子的概率也会变成0。为了解决这个问题引入了马尔科夫假设，即当前单词出现的概率只与前面n个单词有关，与后面的单词无关：

当n=1时：
$$
p(S)=\prod_{i=1}^np(w_i)
$$
当n=2时：
$$
p(S)=p(w_1)p(w_2|w_1)p(w_3|w_2)...p(w_n|w_{n-1})
$$
n-gram的缺点在于无法利用距离大于n的词汇信息，也无法计算词汇的相似度，对于未登录的词(OOV)也没办法处理

NNLM第一次提出词向量的概念，将文本用稠密、低维、连续的向量表达。

+ 输入层：

  输入是前n-1个单词，目标是预测第n个单词，单词输入到模型中时是是这个词的One-Hot向量，然后将这n-1个(V,1)的向量按列拼接成( (n-1)*m, 1)的向量x，其中m是词向量的维度

+ 隐藏层：
  $$
  hidden = tanh(Hx+b)
  $$
  其中H的形状为(h, (n-1)*m)，b的形状为(h, 1)，输出的hidden形状为：(h, 1)

+ 输出层：
  $$
  y = d+Wx+Utanh(Hx+b)
  $$
  其中W的形状为：(V, (n-1)*m),d的形状为(V, 1)，U的形状为：(V, h)

  输出的形状为：(V, 1)

+ 优化目标函数为：
  $$
  L(y, \hat{y})=softmax(y)
  $$
  

































