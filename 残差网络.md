# 残差网络

## 1. 网络退化问题

在神经网络可以收敛的前提下，**随着网络深度增加，网络的表现先是逐渐增加至饱和，然后迅速下降**。

需要注意，网络退化问题不是过拟合导致的，即便在模型训练过程中，同样的训练轮次下，退化的网络也比稍浅层的网络的训练错误更高。

这一点并不符合常理：如果存在某个 $K$ 层的网络 $f$是当前最优的网络，那么可以构造一个更深的网络，其最后几层仅是该网络$f$ 第$K$ 层输出的恒等映射(Identity Mapping)，就可以取得与![[公式]] $f$一致的结果；也许$K$ 还不是所谓“最佳层数”，那么更深的网络就可以取得更好的结果。总而言之，与浅层网络相比，更深的网络的表现不应该更差。因此，一个合理的猜测就是，**对神经网络来说，恒等映射并不容易拟合**。

## 2. 残差网络的定义

$$
z^{(l)}=H(a^{(l-1)})=a^{(l-1)}+F(a^{(l-1)})
$$

在前向传播时，**输入信号可以从任意低层直接传播到高层**。由于包含了一个天然的恒等映射，**一定程度上可以解决网络退化问题**。反向传播时，**错误信号可以不经过任何中间权重矩阵变换直接传播到低层，一定程度上可以缓解梯度弥散问题（即便中间层矩阵权重很小，梯度也基本不会消失）**。



